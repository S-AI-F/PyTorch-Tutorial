{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Tensors\n",
    "Tensors are multidimentional arrays. PyTorch tensors are similar to NumPy's n-dimentional arrays.But, PyTorch tensors can be used on a GPU to accelarate computing.\n",
    "PyTorch supports multiple types of tensors, including:\n",
    "* FloatTensor: 32-bit float\n",
    "* DoubleTensor: 64-bit float\n",
    "* HalfTensor: 16-bit float\n",
    "* IntTensor: 32-bit int\n",
    "* LongTensor: 64-bit int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# initializea numpy array\n",
    "a = np.array(1)\n",
    "# Initialize a tensor\n",
    "b = torch.tensor(1)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n",
      "3\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# intilaizing 2 arrays\n",
    "a = np.array(2)\n",
    "b = np.array(1)\n",
    "print(a,b)\n",
    "# addition\n",
    "print(a+b)\n",
    "# substraction\n",
    "print(a-b)\n",
    "# multiplication\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor(1)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# Initializing 2 tensors\n",
    "a = torch.tensor(2)\n",
    "b = torch.tensor(1)\n",
    "print(a,b)\n",
    "# addition\n",
    "print(a+b)\n",
    "# substraction\n",
    "print(a-b)\n",
    "# multiplication\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct an uninitialized matrix of 5x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1112e-38, 9.5511e-39, 1.0102e-38],\n",
      "        [1.0286e-38, 1.0194e-38, 9.6429e-39],\n",
      "        [9.2755e-39, 9.1837e-39, 9.3674e-39],\n",
      "        [1.0745e-38, 1.0653e-38, 9.5510e-39],\n",
      "        [1.0561e-38, 1.0194e-38, 1.1112e-38]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a randomly initialized matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8532, 0.5282, 0.5074],\n",
      "        [0.0581, 0.5695, 0.7304],\n",
      "        [0.8216, 0.5241, 0.7374],\n",
      "        [0.3378, 0.5755, 0.5188],\n",
      "        [0.8280, 0.5067, 0.7876]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a matrix filled with zeros and of dtyp long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5,3, dtype=torch.long) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random seed for pytorch and initializing two tensors\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(3,3)\n",
    "b = torch.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6040,  0.6637,  1.0438],\n",
      "        [ 1.3406, -2.8127, -1.1753],\n",
      "        [ 3.1662,  0.6841,  1.2788]]) \n",
      "\n",
      "tensor([[ 0.0693, -0.4061, -0.5749],\n",
      "        [-0.8800,  0.5669,  0.8026],\n",
      "        [ 1.2502, -1.9601, -0.3555]]) \n",
      "\n",
      "tensor([[ 0.4576,  0.2724,  0.3367],\n",
      "        [-1.3636,  1.7743,  1.1446],\n",
      "        [ 0.3243,  2.8696,  2.7954]]) \n",
      "\n",
      "tensor([[ 1.2594,  0.2408,  0.2897],\n",
      "        [ 0.2075,  0.6645,  0.1884],\n",
      "        [ 2.3051, -0.4826,  0.5649]])\n"
     ]
    }
   ],
   "source": [
    "# matrix addition\n",
    "print(torch.add(a,b), '\\n')\n",
    "\n",
    "# matrix subtraction\n",
    "print(torch.sub(a,b), '\\n')\n",
    "\n",
    "# matrix multiplication\n",
    "print(torch.mm(a,b), '\\n')\n",
    "\n",
    "# matrix division\n",
    "print(torch.div(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's apply a matrix transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863],\n",
      "        [ 2.2082, -0.6380,  0.4617]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.2303,  2.2082],\n",
       "        [ 0.1288, -1.1229, -0.6380],\n",
       "        [ 0.2345, -0.1863,  0.4617]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original matrix\n",
    "print(a, '\\n')\n",
    "\n",
    "# matrix transpose\n",
    "torch.t(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing 2 tensors\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[5,6],[7,8]])\n",
    "print(a, '\\n')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenating vertically\n",
    "torch.cat((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 5, 6],\n",
       "        [3, 4, 7, 8]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenating horizontally\n",
    "torch.cat((a,b),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863,  2.2082, -0.6380]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the random seed for pytorch\n",
    "torch.manual_seed(42)\n",
    "# initializing tensor\n",
    "a = torch.randn(2,4)\n",
    "print(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the .reshape() function and pass the required shape as a parameter. Let’s try to convert the above tensor of shape (2,4) to a tensor of shape (1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863,  2.2082, -0.6380]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping tensor\n",
    "b = a.reshape(1,8)\n",
    "print(b)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert NumPy arays to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# initializing a numpy array\n",
    "a = np.array([[1,2],[3,4]])\n",
    "print(a, '\\n')\n",
    "\n",
    "# converting the numpy array to tensor\n",
    "tensor = torch.from_numpy(a)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common PyTorch modules\n",
    "## Autograd module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing a tensor\n",
    "a = torch.ones((2,2), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n",
      "        [6., 6.]], grad_fn=<AddBackward0>) tensor(6., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# performing operations on the tensor\n",
    "b = a + 5\n",
    "c = b.mean()\n",
    "print(b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the derivative of c w.r.t. a will be ¼ and hence the gradient matrix will be 0.25. Let’s verify this using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "# back propagating\n",
    "c.backward()\n",
    "\n",
    "# computing gradients\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim module\n",
    "The Optim module in PyTorch has pre-written codes for most of the optimizers that are used while building a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the optim module\n",
    "from torch import optim\n",
    "\n",
    "# adam\n",
    "## adam = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn Module\n",
    "The autograd module in PyTorch helps us define computation graphs as we proceed in the model. But, just using the autograd module can be low-level when we are dealing with a complex neural network.\n",
    "\n",
    "In those cases, we can make use of the nn module. This defines a set of functions, similar to the layers of a neural network, which takes the input from the previous state and produces an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch \n",
    "## Building Neural Network with PyTorch v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the inupt and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 0., 1.]]) \n",
      "\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "#Input tensor\n",
    "X = torch.Tensor([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y = torch.Tensor([[1],[1],[0]])\n",
    "\n",
    "print(X, '\\n')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the sigmoid function that will act as activation function and the derivative of the sigmoid function which will help for the backpropagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function/\n",
    "def derivatives_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the parameters of our model: the number of epochs, learning rate, weights, biases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable initialization\n",
    "epoch = 1000 # setting training iterations\n",
    "lr = 0.1 # setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 # number of hidden layers\n",
    "output_neurons = 1 # number of neurons in output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=torch.randn(inputlayer_neurons, hiddenlayer_neurons).type(torch.FloatTensor)\n",
    "bh=torch.randn(1, hiddenlayer_neurons).type(torch.FloatTensor)\n",
    "wout=torch.randn(hiddenlayer_neurons, output_neurons)\n",
    "bout=torch.randn(1, output_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  i in range(epoch):\n",
    "    # Forward propagation\n",
    "    hidden_layer_input1 = torch.mm(X, wh)\n",
    "    hidden_layer_input = hidden_layer_input1 + bh\n",
    "    hidden_layer_activations = sigmoid(hidden_layer_input)\n",
    "    \n",
    "    output_layer_input1 = torch.mm(hidden_layer_activations, wout)\n",
    "    output_layer_input = output_layer_input1 + bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # Backpropagation\n",
    "    E = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n",
    "    d_output = E * slope_output_layer\n",
    "    Error_at_hidden_layer = torch.mm(d_output, wout.t())\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += torch.mm(hidden_layer_activations.t(), d_output) *lr\n",
    "    bout += d_output.sum() *lr\n",
    "    wh += torch.mm(X.t(), d_hiddenlayer) *lr\n",
    "    bh += d_output.sum() *lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]]) \n",
      "\n",
      "predicted :\n",
      " tensor([[0.9943],\n",
      "        [0.9362],\n",
      "        [0.0915]])\n"
     ]
    }
   ],
   "source": [
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28310293.388650276\n",
      "1 24046455.947607122\n",
      "2 22485748.33202292\n",
      "3 20449816.26516888\n",
      "4 17098542.74024948\n",
      "5 12708479.319280751\n",
      "6 8609402.027216952\n",
      "7 5462124.682564227\n",
      "8 3430326.340406754\n",
      "9 2204568.6675023\n",
      "10 1493119.928472691\n",
      "11 1072276.3243737863\n",
      "12 813773.3048788816\n",
      "13 645344.4338662495\n",
      "14 528451.0129421465\n",
      "15 442429.13691785757\n",
      "16 376134.80888993223\n",
      "17 323226.900301171\n",
      "18 279952.71777665603\n",
      "19 243916.1059332097\n",
      "20 213506.65311874566\n",
      "21 187649.65123356023\n",
      "22 165513.35636200514\n",
      "23 146454.19939037465\n",
      "24 129970.45396461268\n",
      "25 115661.10143721373\n",
      "26 103173.31128772677\n",
      "27 92254.4870180487\n",
      "28 82675.08478930833\n",
      "29 74246.21033616638\n",
      "30 66812.73378047648\n",
      "31 60238.35651494601\n",
      "32 54405.91963871397\n",
      "33 49221.13063303138\n",
      "34 44604.13049982973\n",
      "35 40483.84899006938\n",
      "36 36798.75356089586\n",
      "37 33496.57185901111\n",
      "38 30532.39838244312\n",
      "39 27868.582626639665\n",
      "40 25470.145375360466\n",
      "41 23307.17061950656\n",
      "42 21353.7582883859\n",
      "43 19586.237084515713\n",
      "44 17984.715498949383\n",
      "45 16532.385672097145\n",
      "46 15212.808881699213\n",
      "47 14013.143707463365\n",
      "48 12921.16111241585\n",
      "49 11925.536407755684\n",
      "50 11016.908593091204\n",
      "51 10186.294360167352\n",
      "52 9426.111080526418\n",
      "53 8729.9440491236\n",
      "54 8091.607365090496\n",
      "55 7505.9430218717425\n",
      "56 6968.241432787458\n",
      "57 6475.370831096534\n",
      "58 6021.609453650605\n",
      "59 5603.38237965968\n",
      "60 5217.726544779616\n",
      "61 4861.707133244476\n",
      "62 4532.8957165692\n",
      "63 4228.8558891175635\n",
      "64 3947.583641145088\n",
      "65 3687.0917510130835\n",
      "66 3445.7211329310526\n",
      "67 3221.9154535620855\n",
      "68 3014.1855952998976\n",
      "69 2821.357933761189\n",
      "70 2642.163640316422\n",
      "71 2475.554500082286\n",
      "72 2320.535432065917\n",
      "73 2176.2167683394346\n",
      "74 2041.7919196718976\n",
      "75 1916.5452678104625\n",
      "76 1799.7555788450977\n",
      "77 1690.8113549933494\n",
      "78 1589.1068188646484\n",
      "79 1494.1750912977402\n",
      "80 1405.4662397556135\n",
      "81 1322.4776254246387\n",
      "82 1244.849980887462\n",
      "83 1172.1849533726368\n",
      "84 1104.1483865262162\n",
      "85 1040.4262931668652\n",
      "86 980.7219605193584\n",
      "87 924.7330983463828\n",
      "88 872.2148332370462\n",
      "89 822.9232019670588\n",
      "90 776.6486563311325\n",
      "91 733.2012554737134\n",
      "92 692.3774703635474\n",
      "93 654.005932678723\n",
      "94 617.9338276564949\n",
      "95 584.0096465848856\n",
      "96 552.084774480714\n",
      "97 522.0394535323644\n",
      "98 493.7497037908193\n",
      "99 467.11297809534017\n",
      "100 442.0196920269931\n",
      "101 418.36671013164175\n",
      "102 396.068927679093\n",
      "103 375.0482671823026\n",
      "104 355.2174481499124\n",
      "105 336.50475483048456\n",
      "106 318.8456233478238\n",
      "107 302.1797170564616\n",
      "108 286.43967965540537\n",
      "109 271.57117062162615\n",
      "110 257.5256130806948\n",
      "111 244.24493750943515\n",
      "112 231.6879157536352\n",
      "113 219.81577479947404\n",
      "114 208.59252662258183\n",
      "115 197.97450953432318\n",
      "116 187.92857164985804\n",
      "117 178.42268876930598\n",
      "118 169.42463488756204\n",
      "119 160.9060620620768\n",
      "120 152.83985922564813\n",
      "121 145.19957590445483\n",
      "122 137.9632543460145\n",
      "123 131.10672140446866\n",
      "124 124.60768003254353\n",
      "125 118.44803552173002\n",
      "126 112.60901788308884\n",
      "127 107.0722222704504\n",
      "128 101.82175107310255\n",
      "129 96.84247189017843\n",
      "130 92.11720024390878\n",
      "131 87.63385876690586\n",
      "132 83.37877324045726\n",
      "133 79.34016001427295\n",
      "134 75.50668513736967\n",
      "135 71.86704935012374\n",
      "136 68.41097125289365\n",
      "137 65.12822599794946\n",
      "138 62.009785405584424\n",
      "139 59.04746823963053\n",
      "140 56.232951604329855\n",
      "141 53.55810326895457\n",
      "142 51.015887697193556\n",
      "143 48.59928187386413\n",
      "144 46.30156627123861\n",
      "145 44.11744807191599\n",
      "146 42.04022884340013\n",
      "147 40.064373286830204\n",
      "148 38.18512240992843\n",
      "149 36.39752129171195\n",
      "150 34.6972102604244\n",
      "151 33.079277878241896\n",
      "152 31.53922855553609\n",
      "153 30.07343302058206\n",
      "154 28.67843531307377\n",
      "155 27.35049198735706\n",
      "156 26.0815409580291\n",
      "157 24.873529493757193\n",
      "158 23.72361965082265\n",
      "159 22.628681246221266\n",
      "160 21.586187669986547\n",
      "161 20.593470211235342\n",
      "162 19.64784844933684\n",
      "163 18.747252668030928\n",
      "164 17.889318423698978\n",
      "165 17.07198865679925\n",
      "166 16.29308671097891\n",
      "167 15.550997491058833\n",
      "168 14.843695537785383\n",
      "169 14.169589875052818\n",
      "170 13.527114941046474\n",
      "171 12.914594075668155\n",
      "172 12.330745272315003\n",
      "173 11.774035244153294\n",
      "174 11.243335992213513\n",
      "175 10.737194646778622\n",
      "176 10.254487372373463\n",
      "177 9.794211759632574\n",
      "178 9.355147586881388\n",
      "179 8.936318784040362\n",
      "180 8.536763617318028\n",
      "181 8.155553296783143\n",
      "182 7.79186767456404\n",
      "183 7.444879545019509\n",
      "184 7.113741372614351\n",
      "185 6.797730696456984\n",
      "186 6.4961685684469135\n",
      "187 6.208378913958981\n",
      "188 5.9336034129723725\n",
      "189 5.671306457213093\n",
      "190 5.420922865355612\n",
      "191 5.181890700798588\n",
      "192 4.953643870724743\n",
      "193 4.735730329768964\n",
      "194 4.527650474133972\n",
      "195 4.328947407334711\n",
      "196 4.139161693283616\n",
      "197 3.9579009738527002\n",
      "198 3.7847931533140224\n",
      "199 3.6194157127933044\n",
      "200 3.461436222433076\n",
      "201 3.3105341655189218\n",
      "202 3.1663777412495735\n",
      "203 3.0286295128126977\n",
      "204 2.8970106769574153\n",
      "205 2.7712559028358665\n",
      "206 2.6510848020660904\n",
      "207 2.53623540756448\n",
      "208 2.4264801771641475\n",
      "209 2.321598815062639\n",
      "210 2.2213468760779174\n",
      "211 2.125505202903044\n",
      "212 2.033904574302478\n",
      "213 1.946331682593826\n",
      "214 1.8626189207852635\n",
      "215 1.7825834991744673\n",
      "216 1.7060641857377252\n",
      "217 1.6328980917194698\n",
      "218 1.5629283694607932\n",
      "219 1.4960347218330208\n",
      "220 1.4320619435895077\n",
      "221 1.37087400783556\n",
      "222 1.3123628185169003\n",
      "223 1.2564071247245983\n",
      "224 1.202875462509006\n",
      "225 1.1516759327161379\n",
      "226 1.1027057369584112\n",
      "227 1.055856589492732\n",
      "228 1.0110398113744319\n",
      "229 0.9681628610064027\n",
      "230 0.9271382601498769\n",
      "231 0.8878912063681443\n",
      "232 0.8503378140044084\n",
      "233 0.8144020999809494\n",
      "234 0.7800181455270675\n",
      "235 0.7471139626050728\n",
      "236 0.715620105470345\n",
      "237 0.6854832525919228\n",
      "238 0.6566403419076423\n",
      "239 0.6290373531074495\n",
      "240 0.6026162884357189\n",
      "241 0.577323069496066\n",
      "242 0.5531144675104447\n",
      "243 0.5299400521663917\n",
      "244 0.5077537085751532\n",
      "245 0.4865151714564852\n",
      "246 0.46618379491555556\n",
      "247 0.4467134893086713\n",
      "248 0.4280734359287723\n",
      "249 0.4102239194052185\n",
      "250 0.393134768694435\n",
      "251 0.37677100540275177\n",
      "252 0.3610979825227868\n",
      "253 0.3460913161678559\n",
      "254 0.33171955955751076\n",
      "255 0.3179543143700214\n",
      "256 0.3047730857949432\n",
      "257 0.292145735128972\n",
      "258 0.2800522416559147\n",
      "259 0.26846668109333527\n",
      "260 0.2573699908777288\n",
      "261 0.24674172635025846\n",
      "262 0.2365588043749947\n",
      "263 0.22680385834038955\n",
      "264 0.21745671947375486\n",
      "265 0.2085029112745909\n",
      "266 0.1999238991072607\n",
      "267 0.19170413500482258\n",
      "268 0.18382854720248368\n",
      "269 0.176281460339471\n",
      "270 0.16905062201096138\n",
      "271 0.1621201945270207\n",
      "272 0.15547919307683916\n",
      "273 0.1491146130207716\n",
      "274 0.1430160730609168\n",
      "275 0.13717031960329829\n",
      "276 0.13156766547275395\n",
      "277 0.12619739369725846\n",
      "278 0.12105036103304154\n",
      "279 0.11611756864142186\n",
      "280 0.11138837059308887\n",
      "281 0.10685480074308543\n",
      "282 0.10250888479166605\n",
      "283 0.09834330077310136\n",
      "284 0.09434913265620995\n",
      "285 0.09052006724108391\n",
      "286 0.08684884685199273\n",
      "287 0.08332909294819982\n",
      "288 0.07995397302092755\n",
      "289 0.07671790217352716\n",
      "290 0.07361489634699164\n",
      "291 0.07063973974680038\n",
      "292 0.06778661319831408\n",
      "293 0.06505059903580244\n",
      "294 0.06242656534450797\n",
      "295 0.0599100676333535\n",
      "296 0.05749714099119227\n",
      "297 0.05518286961041962\n",
      "298 0.052962843483169236\n",
      "299 0.05083339628375538\n",
      "300 0.04879128791962776\n",
      "301 0.04683236529863154\n",
      "302 0.04495338820995585\n",
      "303 0.04315075236165722\n",
      "304 0.041421622733539235\n",
      "305 0.03976293218344111\n",
      "306 0.038171470220984105\n",
      "307 0.03664490637172546\n",
      "308 0.03518026837162032\n",
      "309 0.03377515438332597\n",
      "310 0.032426911954000805\n",
      "311 0.03113325132288412\n",
      "312 0.029892026215536084\n",
      "313 0.028701055887652063\n",
      "314 0.027558244958424304\n",
      "315 0.026461701837354898\n",
      "316 0.025409264266432516\n",
      "317 0.024399352102369994\n",
      "318 0.023430310215607004\n",
      "319 0.022500208262409276\n",
      "320 0.02160756938333391\n",
      "321 0.020750906458387564\n",
      "322 0.019928631911722477\n",
      "323 0.01913946641453542\n",
      "324 0.018381998054011455\n",
      "325 0.017654891949707787\n",
      "326 0.016956956938189903\n",
      "327 0.016287098174217182\n",
      "328 0.01564400010165083\n",
      "329 0.015026694034256551\n",
      "330 0.01443409009644609\n",
      "331 0.01386516324046604\n",
      "332 0.013318999680905127\n",
      "333 0.012794725451982476\n",
      "334 0.012291301676112214\n",
      "335 0.01180792355298828\n",
      "336 0.01134387896070069\n",
      "337 0.010898288046881626\n",
      "338 0.010470444567694224\n",
      "339 0.010059639632339398\n",
      "340 0.009665152546135744\n",
      "341 0.009286367160005408\n",
      "342 0.008922635394744187\n",
      "343 0.008573314239759964\n",
      "344 0.008237870832334104\n",
      "345 0.007915744747089858\n",
      "346 0.007606356962292243\n",
      "347 0.0073092116608222246\n",
      "348 0.007023857178680594\n",
      "349 0.0067497701149363005\n",
      "350 0.006486543565115377\n",
      "351 0.006233726841983504\n",
      "352 0.005990855075558827\n",
      "353 0.005757574265260811\n",
      "354 0.005533499089667283\n",
      "355 0.005318287830008013\n",
      "356 0.005111520200142454\n",
      "357 0.004912889861253385\n",
      "358 0.004722078975107994\n",
      "359 0.004538790445403442\n",
      "360 0.0043627023669872015\n",
      "361 0.0041935185614392825\n",
      "362 0.00403098021434877\n",
      "363 0.0038748184584408507\n",
      "364 0.0037247911928244325\n",
      "365 0.003580649900327931\n",
      "366 0.003442147317701428\n",
      "367 0.0033090628955435605\n",
      "368 0.003181185170458646\n",
      "369 0.003058338508216597\n",
      "370 0.002940272849866982\n",
      "371 0.0028268159982238923\n",
      "372 0.0027177864922227945\n",
      "373 0.0026130167203544815\n",
      "374 0.0025123370172499626\n",
      "375 0.00241557904237818\n",
      "376 0.0023225963060926526\n",
      "377 0.0022332267053448377\n",
      "378 0.002147338221070115\n",
      "379 0.002064794436361049\n",
      "380 0.001985461877494158\n",
      "381 0.0019092105142511258\n",
      "382 0.0018359185430808552\n",
      "383 0.0017654750353655374\n",
      "384 0.0016977650695291775\n",
      "385 0.0016326786368142765\n",
      "386 0.0015701214886048522\n",
      "387 0.0015099877654186373\n",
      "388 0.001452179824761437\n",
      "389 0.0013966080332459205\n",
      "390 0.001343185739721047\n",
      "391 0.0012918327664256921\n",
      "392 0.0012424605736183454\n",
      "393 0.0011949990132137432\n",
      "394 0.001149368683666982\n",
      "395 0.0011055002520298607\n",
      "396 0.0010633238416970603\n",
      "397 0.0010227712288964409\n",
      "398 0.0009837841236928005\n",
      "399 0.0009462980286372648\n",
      "400 0.0009102541296184455\n",
      "401 0.000875599490990205\n",
      "402 0.00084227779614622\n",
      "403 0.0008102371900474876\n",
      "404 0.0007794306935038913\n",
      "405 0.000749806381361978\n",
      "406 0.000721317911366683\n",
      "407 0.0006939213698821816\n",
      "408 0.0006675773432544681\n",
      "409 0.0006422423907552627\n",
      "410 0.0006178789709062632\n",
      "411 0.0005944494209920626\n",
      "412 0.0005719160243741544\n",
      "413 0.0005502491095303899\n",
      "414 0.0005294079121458618\n",
      "415 0.0005093634933576877\n",
      "416 0.0004900855576158611\n",
      "417 0.00047154419289362366\n",
      "418 0.00045371105069026826\n",
      "419 0.00043655850654525635\n",
      "420 0.0004200604710583081\n",
      "421 0.0004041934109569097\n",
      "422 0.0003889317372586682\n",
      "423 0.00037424977377584765\n",
      "424 0.000360127752923214\n",
      "425 0.0003465430311892471\n",
      "426 0.0003334755300181855\n",
      "427 0.0003209051341070759\n",
      "428 0.0003088128410259647\n",
      "429 0.00029718089315049613\n",
      "430 0.00028599067443226965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 0.0002752255035583401\n",
      "432 0.0002648689137513053\n",
      "433 0.0002549053318723759\n",
      "434 0.0002453202650721338\n",
      "435 0.00023609805085858423\n",
      "436 0.000227226615846206\n",
      "437 0.00021869067259245586\n",
      "438 0.0002104786807829538\n",
      "439 0.00020257831854698642\n",
      "440 0.00019497581673589534\n",
      "441 0.00018766134824538733\n",
      "442 0.00018062336627349407\n",
      "443 0.00017385153431205287\n",
      "444 0.0001673357476930672\n",
      "445 0.00016106604967005287\n",
      "446 0.00015503335266641978\n",
      "447 0.00014922813060062033\n",
      "448 0.00014364231137341298\n",
      "449 0.00013826730148892643\n",
      "450 0.00013309478783933551\n",
      "451 0.00012811731329369504\n",
      "452 0.00012332742537102994\n",
      "453 0.00011871813161587876\n",
      "454 0.00011428248621431217\n",
      "455 0.00011001388007456255\n",
      "456 0.00010590620062950182\n",
      "457 0.0001019525373885689\n",
      "458 9.814756970620537e-05\n",
      "459 9.448593634173123e-05\n",
      "460 9.096207196079588e-05\n",
      "461 8.757045166464037e-05\n",
      "462 8.430595253143273e-05\n",
      "463 8.116410971504952e-05\n",
      "464 7.814024572888228e-05\n",
      "465 7.523002312246211e-05\n",
      "466 7.242887822334919e-05\n",
      "467 6.973274341267073e-05\n",
      "468 6.713783938016874e-05\n",
      "469 6.463990456310922e-05\n",
      "470 6.223582185149802e-05\n",
      "471 5.992163028754656e-05\n",
      "472 5.76944152217632e-05\n",
      "473 5.5550203431508165e-05\n",
      "474 5.348628372796268e-05\n",
      "475 5.1499581873875054e-05\n",
      "476 4.958718893357846e-05\n",
      "477 4.7746227448897105e-05\n",
      "478 4.5974028292150636e-05\n",
      "479 4.426803933975323e-05\n",
      "480 4.2625818392849236e-05\n",
      "481 4.104507429963125e-05\n",
      "482 3.952309675873129e-05\n",
      "483 3.805795570921407e-05\n",
      "484 3.664750782296463e-05\n",
      "485 3.528966420372286e-05\n",
      "486 3.398255563305972e-05\n",
      "487 3.272409132246788e-05\n",
      "488 3.1512595823742015e-05\n",
      "489 3.0346128249224937e-05\n",
      "490 2.9223104399004403e-05\n",
      "491 2.8141961120901352e-05\n",
      "492 2.7101041562610094e-05\n",
      "493 2.609881867299161e-05\n",
      "494 2.51338653623664e-05\n",
      "495 2.4204778536500622e-05\n",
      "496 2.3310344000773842e-05\n",
      "497 2.244912009231838e-05\n",
      "498 2.161984057842342e-05\n",
      "499 2.082139233739504e-05\n"
     ]
    }
   ],
   "source": [
    "# N = batch size ; D_in is input dimension ; H = hidden dimension; D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly intialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h,0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with repect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network with PyTorch v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = \"cpu\"\n",
    "\n",
    "# N = batch size ; D_in is input dimension ; H = hidden dimension; D_out = output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 648.1469116210938\n",
      "199 3.491764545440674\n",
      "299 0.03566964715719223\n",
      "399 0.0007340770680457354\n",
      "499 8.137375698424876e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn module\n",
    "In pytorch, the `nn` package enables the definition of neural network layers, called **modules**. The `nn` package fot `pytorch` is like `keras` for `Tensorflow`. A module receives input Tensors and computes output Tensors. The `nn` package also provides some commonly used loss functions.\n",
    "Here is an example if using the `nn` package to implement a two-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.33319354057312\n",
      "199 0.07260344177484512\n",
      "299 0.007553726434707642\n",
      "399 0.001089669531211257\n",
      "499 0.000166371013619937\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# use the nn package to define our model as a sequence of layers.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
